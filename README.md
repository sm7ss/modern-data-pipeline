# ğŸ”„ Pipeline ETL - Polars + PostgreSQL + DuckDB

[![Estado](https://img.shields.io/badge/Estado-ğŸš§_Desarrollo_Activo-orange)]()
[![Stack](https://img.shields.io/badge/Stack-Python_Data_Engineering-blue)]()
[![ETL](https://img.shields.io/badge/ETL-Polars_|_PostgreSQL_|_DuckDB-green)]()

> **ğŸ’¡ Proyecto de IngenierÃ­a de Datos** - Un pipeline ETL modular que explora las mejores herramientas para cada etapa del proceso de datos.

## ğŸ¯ Objetivo del Proyecto

Implementar un **pipeline ETL robusto** utilizando herramientas modernas donde cada una brilla en su especialidad:

- **ğŸ”„ Polars**: Transformaciones ultra-rÃ¡pidas de datos
- **ğŸ˜ PostgreSQL**: Almacenamiento persistente y estructurado  
- **ğŸ¦† DuckDB**: AnÃ¡lisis analÃ­tico rÃ¡pido y eficiente
- **ğŸ³ Docker**: ContenedorizaciÃ³n de la base de datos

## ğŸ› ï¸ Stack TecnolÃ³gico Actual

### **Lenguaje Principal**

![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)

### **Procesamiento de Datos**

![Polars](https://img.shields.io/badge/Polars-CD7929?logo=rust&logoColor=white)
![PyArrow](https://img.shields.io/badge/PyArrow-0C0D0D?logo=apachearrow&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-FFF000?logo=duckdb&logoColor=black)

### **Base de Datos & Infraestructura**

![PostgreSQL](https://img.shields.io/badge/PostgreSQL-4169E1?logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)

### **Calidad & ValidaciÃ³n**

![Pydantic](https://img.shields.io/badge/Pydantic-E92063?logo=pydantic&logoColor=white)
![Pandera](https://img.shields.io/badge/Pandera-3EB049?logo=python&logoColor=white)

### **Utilidades**

![YAML](https://img.shields.io/badge/YAML-CB171E?logo=yaml&logoColor=white)
![Logging](https://img.shields.io/badge/Logging-000000?logo=python&logoColor=white)
![psutil](https://img.shields.io/badge/psutil-3A75BD?logo=python&logoColor=white)

## ğŸ“ˆ Estado Actual del Desarrollo

### **âœ… Implementando**

- [ ] ConfiguraciÃ³n YAML con validaciÃ³n Pydantic
- [ ] GestiÃ³n de recursos del sistema con psutil
- [ ] Lectura eficiente de archivos CSV/Parquet
- [ ] Contenedor Docker para PostgreSQL

### **ğŸš§ En Progreso**

- [ ] Sistema de logging estructurado
- [ ] NormalizaciÃ³n robusta de formatos de fecha
- [ ] ValidaciÃ³n de calidad de datos con Pandera
- [ ] ConexiÃ³n y escritura a PostgreSQL
- [ ] IntegraciÃ³n con DuckDB para anÃ¡lisis

### **ğŸ“… PrÃ³ximos Objetivos**

- [ ] Transformaciones bÃ¡sicas con Polars
- [ ] Pipeline ETL end-to-end funcionando
- [ ] OptimizaciÃ³n de memoria con PyArrow
- [ ] AnÃ¡lisis demostrativo con DuckDB

## ğŸš€ InstalaciÃ³n y Uso

```bash
# Clonar el proyecto
git clone https://github.com/sm7ss/modern-data-pipeline.git
cd modern-data-pipeline

# Instalar dependencias (ejemplo)
pip install -r requirements.txt

# Ejecutar contenedor de PostgreSQL
docker-compose up -d

# Ejecutar pipeline
python main.py
```

## ğŸ¯ Casos de Uso Explorados

- ğŸ”„ Transformaciones ETL con sintaxis moderna de Polars
- ğŸ“Š AnÃ¡lisis con la velocidad de DuckDB
- ğŸ³ Infraestructura reproducible con Docker
- âœ… ValidaciÃ³n de datos con Pydantic + Pandera
- ğŸ“ˆ Metadatos y performance con PyArrow

## ğŸ¤ Contribuciones y Feedback

Si tienes sugerencias para:

- Mejoras en la arquitectura
- Optimizaciones de performance
- Mejores prÃ¡cticas de ingenierÃ­a de datos
- Ideas para nuevas features

Â¡Tu feedback es super bienvenido! ğŸ’«

## ğŸ‘©â€ğŸ’» Sobre la Desarrolladora

ğŸ’­ **Â¿Por quÃ© este stack?** Cada herramienta fue elegida por su especialidad: Polars para velocidad, PostgreSQL para persistencia, DuckDB para anÃ¡lisis.
